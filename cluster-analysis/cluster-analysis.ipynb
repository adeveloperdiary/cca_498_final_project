{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "import nltk\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../raw_data/local-dev/Questions.csv\", encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Title'].fillna(\"None\", inplace=True)\n",
    "data['Score'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'html5lib')\n",
    "    return soup.getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['combined_title_body'] = data['Title'].map(str) + data['Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.loc[:, 'combined_title_body'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Migrating to Twitter API version 1.1 (?) [duplicated]I am quite new to Twitter API. I have updated Tweepy. I don't know what is wrong with this code and how to fix it to make it work for new version of Twitter API:\\n\\nimport oauth, tweepy \\nfrom time import sleep\\n\\n#stars is confident information\\nusername = \"*******\"\\npassword = \"***********\"\\nauth = tweepy.BasicAuthHandler(username, password)\\napi = tweepy.API(auth)\\n\\napi.update_status('hello from tweepy!')\\n\\n\\nTerminal is showing me this:\\n\\n$ python py/twi.py\\nTraceback (most recent call last):\\n  File \"py/twi.py\", line 11, in <module>\\n    api.update_status('hello from tweepy!')\\n  File \"/usr/lib/python2.7/dist-packages/tweepy/binder.py\", line 179, in _call\\n    return method.execute()\\n  File \"/usr/lib/python2.7/dist-packages/tweepy/binder.py\", line 162, in execute\\n    raise TweepError(error_msg, resp)\\n tweepy.error.TweepError: [{'message': 'The Twitter REST API v1 is no longer active. Please migrate to     API v1.1. https://dev.twitter.com/docs/api/1.1/overview.', 'code': 68}]\\n\\n\\nPlease help.\\n\n",
       "Name: combined_title_body, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adantonison/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adantonison/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/adantonison/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using a WordNetLemmatizer to improve the quality of the clustering\n",
    "* See here for more inofrmation http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "def cond_tokenize(t):\n",
    "    if t is None:\n",
    "        return []\n",
    "    else:\n",
    "        return [lem.lemmatize(w.lower()) for w in word_tokenize(t)]\n",
    "\n",
    "p = Pool(8)\n",
    "tokens = list(p.imap(cond_tokenize, corpus))\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once tokenized, I now want to join all of the text back together into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_tokens = [\" \".join(sent) for sent in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here I am using the sklearn vectorizer to convert the raw documents to a matrix of TF-IDF features\n",
    "* See http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(pure_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>word</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>59334</td>\n",
       "      <td>python</td>\n",
       "      <td>1.549666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>23364</td>\n",
       "      <td>code</td>\n",
       "      <td>1.964269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>76044</td>\n",
       "      <td>using</td>\n",
       "      <td>1.992192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>45175</td>\n",
       "      <td>like</td>\n",
       "      <td>2.060550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>40576</td>\n",
       "      <td>import</td>\n",
       "      <td>2.180031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>33583</td>\n",
       "      <td>file</td>\n",
       "      <td>2.187871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>27653</td>\n",
       "      <td>def</td>\n",
       "      <td>2.247328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>77437</td>\n",
       "      <td>want</td>\n",
       "      <td>2.249768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>57690</td>\n",
       "      <td>print</td>\n",
       "      <td>2.275569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>62542</td>\n",
       "      <td>return</td>\n",
       "      <td>2.340274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    word       idf\n",
       "30   59334  python  1.549666\n",
       "11   23364  code    1.964269\n",
       "163  76044  using   1.992192\n",
       "184  45175  like    2.060550\n",
       "15   40576  import  2.180031\n",
       "35   33583  file    2.187871\n",
       "112  27653  def     2.247328\n",
       "289  77437  want    2.249768\n",
       "215  57690  print   2.275569\n",
       "47   62542  return  2.340274"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfs = pd.DataFrame([[v, k] for k, v in vectorizer.vocabulary_.items()], columns=['id', 'word']).sort_values('id')\n",
    "idfs['idf'] = vectorizer.idf_\n",
    "idfs.sort_values('idf').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Going to compress using a SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd = TruncatedSCD(n_components=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
